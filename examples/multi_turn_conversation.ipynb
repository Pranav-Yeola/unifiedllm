{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Turn Conversations with UnifiedLLM\n",
    "\n",
    "This notebook demonstrates how to build conversational applications that maintain context across multiple turns.\n",
    "\n",
    "**Topics covered:**\n",
    "- Building a message history\n",
    "- Creating a chat loop function\n",
    "- Scripted conversations for testing\n",
    "- Optional interactive chat\n",
    "- Common mistakes to avoid\n",
    "\n",
    "**Use cases:**\n",
    "- Chatbots that remember conversation history\n",
    "- Interactive tutoring systems\n",
    "- Customer service assistants\n",
    "- Code review assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unifiedllm import LLM\n",
    "from unifiedllm.errors import MissingAPIKeyError, ProviderAPIError\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Set API key directly in notebook (if not already set via terminal)\n",
    "# Uncomment and add your key if the export command didn't work:\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Check if API key is set\n",
    "if not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    print(\"‚ö†Ô∏è  GEMINI_API_KEY environment variable is not set!\")\n",
    "    print(\"\")\n",
    "    print(\"Option 1 - Set via terminal:\")\n",
    "    print(\"  export GEMINI_API_KEY='your-api-key'\")\n",
    "    print(\"\")\n",
    "    print(\"Option 2 - Set in this notebook (uncomment line above):\")\n",
    "    print(\"  os.environ['GEMINI_API_KEY'] = 'your-api-key'\")\n",
    "else:\n",
    "    print(\"‚úÖ GEMINI_API_KEY is set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Multi-Turn Conversations\n",
    "\n",
    "In a multi-turn conversation:\n",
    "1. Maintain a list of messages\n",
    "2. Each message has a `role` (\"user\" or \"model\") and `content` (the text)\n",
    "3. After each turn, append both the user's message and the model's response to the list\n",
    "4. Pass the entire message history to maintain context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM client\n",
    "llm = LLM(\n",
    "    provider=\"gemini\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Chat Loop Function\n",
    "\n",
    "Let's create a reusable function that handles multi-turn conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversation(llm, user_messages, system_prompt=None):\n",
    "    \"\"\"\n",
    "    Conduct a multi-turn conversation with the LLM.\n",
    "    \n",
    "    Args:\n",
    "        llm: The LLM instance\n",
    "        user_messages: List of user messages (strings)\n",
    "        system_prompt: Optional system prompt to set behavior\n",
    "    \n",
    "    Returns:\n",
    "        List of all messages (user + model)\n",
    "    \"\"\"\n",
    "    # Set system prompt if provided\n",
    "    if system_prompt:\n",
    "        llm.system_prompt(system_prompt)\n",
    "    \n",
    "    # Initialize message history\n",
    "    messages = []\n",
    "    \n",
    "    # Process each user message\n",
    "    for user_msg in user_messages:\n",
    "        print(f\"\\nüë§ User: {user_msg}\")\n",
    "        \n",
    "        # Add user message to history\n",
    "        messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "        \n",
    "        # Get model response\n",
    "        try:\n",
    "            response = llm.chat(messages=messages)\n",
    "            \n",
    "            # Add model response to history\n",
    "            messages.append({\"role\": \"model\", \"content\": response.text})\n",
    "            \n",
    "            print(f\"ü§ñ Assistant: {response.text}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "            break\n",
    "    \n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Scripted Conversation\n",
    "\n",
    "Let's demonstrate a teaching scenario where context matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversation flow\n",
    "user_inputs = [\n",
    "    \"I want to learn about Python functions.\",\n",
    "    \"Can you show me a simple example?\",\n",
    "    \"How do I add parameters to it?\",\n",
    "    \"What's a return value?\"\n",
    "]\n",
    "\n",
    "# Run the conversation\n",
    "conversation = chat_conversation(\n",
    "    llm,\n",
    "    user_inputs,\n",
    "    system_prompt=\"You are a patient Python tutor. Keep explanations concise and use code examples.\"\n",
    ")\n",
    "\n",
    "print(f\"\\n\\nüìù Total messages in conversation: {len(conversation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Problem-Solving Conversation\n",
    "\n",
    "Let's see how the model maintains context while debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging_conversation = [\n",
    "    \"My Python code has an error: 'list index out of range'. What does this mean?\",\n",
    "    \"Here's the code: numbers = [1, 2, 3]; print(numbers[5])\",\n",
    "    \"How can I check the length of a list before accessing it?\"\n",
    "]\n",
    "\n",
    "messages = chat_conversation(\n",
    "    llm,\n",
    "    debugging_conversation,\n",
    "    system_prompt=\"You are a helpful debugging assistant. Provide clear, practical solutions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Message History\n",
    "\n",
    "It's useful to inspect the complete message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_messages(messages):\n",
    "    \"\"\"\n",
    "    Pretty print the message history.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CONVERSATION HISTORY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, msg in enumerate(messages, 1):\n",
    "        role_emoji = \"üë§\" if msg[\"role\"] == \"user\" else \"ü§ñ\"\n",
    "        role_label = \"User\" if msg[\"role\"] == \"user\" else \"Assistant\"\n",
    "        \n",
    "        print(f\"\\n{i}. {role_emoji} {role_label}:\")\n",
    "        print(f\"   {msg['content'][:200]}...\")  # Show first 200 chars\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Print the conversation history\n",
    "print_messages(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Chat (Optional)\n",
    "\n",
    "**Note:** This cell uses `input()` for interactive chat. It won't run automatically.\n",
    "Run it manually if you want to have a real-time conversation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Interactive chat loop\n",
    "# Uncomment and run this cell to chat interactively\n",
    "\n",
    "def interactive_chat(llm, system_prompt=None):\n",
    "    \"\"\"\n",
    "    Start an interactive chat session.\n",
    "    Type 'quit' or 'exit' to end the conversation.\n",
    "    \"\"\"\n",
    "    if system_prompt:\n",
    "        llm.system_prompt(system_prompt)\n",
    "    \n",
    "    messages = []\n",
    "    print(\"ü§ñ Chat started! Type 'quit' or 'exit' to end.\\n\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"üë§ You: \")\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit']:\n",
    "            print(\"\\nüëã Chat ended. Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        try:\n",
    "            response = llm.chat(messages=messages)\n",
    "            messages.append({\"role\": \"model\", \"content\": response.text})\n",
    "            print(f\"ü§ñ Assistant: {response.text}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\\n\")\n",
    "            break\n",
    "    \n",
    "    return messages\n",
    "\n",
    "# Run the interactive chat\n",
    "interactive_messages = interactive_chat(\n",
    "    llm,\n",
    "    system_prompt=\"You are a friendly AI assistant.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Mistakes to Avoid\n",
    "\n",
    "Here are some common pitfalls when building multi-turn conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 1: Invalid Role Names\n",
    "\n",
    "Only \"user\" and \"model\" are valid roles. Other roles will raise a `ValueError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå This will fail - invalid role\n",
    "try:\n",
    "    bad_messages = [\n",
    "        {\"role\": \"assistant\", \"content\": \"Hello\"}  # Should be \"model\", not \"assistant\"\n",
    "    ]\n",
    "    response = llm.chat(messages=bad_messages)\n",
    "except ValueError as e:\n",
    "    print(f\"‚ùå ValueError: {e}\")\n",
    "\n",
    "# ‚úÖ Correct way\n",
    "good_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello\"}\n",
    "]\n",
    "response = llm.chat(messages=good_messages)\n",
    "print(f\"‚úÖ Correct: {response.text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 2: Forgetting to Append Model Responses\n",
    "\n",
    "If you don't add the model's responses to the message list, it will lose context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ùå Bad: Context is lost\n",
    "messages = [{\"role\": \"user\", \"content\": \"My name is Alice.\"}]\n",
    "response1 = llm.chat(messages=messages)\n",
    "print(f\"Turn 1: {response1.text[:80]}\")\n",
    "\n",
    "# Not adding response1 to messages!\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's my name?\"})  # Model won't remember\n",
    "response2 = llm.chat(messages=messages)\n",
    "print(f\"\\nTurn 2 (without context): {response2.text[:80]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# ‚úÖ Good: Context is maintained\n",
    "messages = [{\"role\": \"user\", \"content\": \"My name is Bob.\"}]\n",
    "response1 = llm.chat(messages=messages)\n",
    "print(f\"Turn 1: {response1.text[:80]}\")\n",
    "\n",
    "# Add model response before next turn\n",
    "messages.append({\"role\": \"model\", \"content\": response1.text})\n",
    "messages.append({\"role\": \"user\", \"content\": \"What's my name?\"})\n",
    "response2 = llm.chat(messages=messages)\n",
    "print(f\"\\nTurn 2 (with context): {response2.text[:80]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistake 3: Missing API Key\n",
    "\n",
    "Always ensure your API key is set before making requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pattern checks for the key before making any requests\n",
    "def safe_chat(llm, messages):\n",
    "    try:\n",
    "        response = llm.chat(messages=messages)\n",
    "        return response\n",
    "    except MissingAPIKeyError:\n",
    "        print(\"‚ùå API key is missing! Please set GOOGLE_API_KEY environment variable.\")\n",
    "        return None\n",
    "    except ProviderAPIError as e:\n",
    "        print(f\"‚ùå API Error: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "result = safe_chat(llm, test_messages)\n",
    "if result:\n",
    "    print(f\"‚úÖ Success: {result.text[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices Summary\n",
    "\n",
    "‚úÖ **Do:**\n",
    "- Always use \"user\" and \"model\" as roles\n",
    "- Append both user messages AND model responses to maintain context\n",
    "- Set a system prompt to guide the assistant's behavior\n",
    "- Handle errors gracefully with try/except blocks\n",
    "- Keep track of message history for debugging\n",
    "\n",
    "‚ùå **Don't:**\n",
    "- Use invalid role names like \"assistant\" or \"system\"\n",
    "- Forget to append model responses to the message list\n",
    "- Make API calls without checking for errors\n",
    "- Pass both `prompt` and `messages` parameters together (use one or the other)\n",
    "\n",
    "**Next steps:**\n",
    "- Try building a simple chatbot for your specific use case\n",
    "- Experiment with different system prompts to change behavior\n",
    "- Check out `provider_comparison.ipynb` to see how different providers handle conversations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".unifiedllm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
