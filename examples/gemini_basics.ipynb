{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemini Basics with UnifiedLLM\n",
    "\n",
    "This notebook demonstrates the basic features of the `unifiedllm` library using Google's Gemini as the provider.\n",
    "\n",
    "**Topics covered:**\n",
    "- Installation and setup\n",
    "- Basic chat with prompts\n",
    "- Understanding ChatResponse fields\n",
    "- Message-based conversations\n",
    "- System prompts\n",
    "- Configuration options (temperature, max_tokens)\n",
    "- Error handling\n",
    "\n",
    "**Why Gemini?** Google's Gemini API offers a generous free tier, making it perfect for learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install unifiedllm-sdk\n",
    "```\n",
    "\n",
    "### API Key\n",
    "You'll need a Google API key with access to Gemini. Get one at [Google AI Studio](https://aistudio.google.com/app/apikey).\n",
    "\n",
    "**Option 1 - Set via terminal (recommended):**\n",
    "```bash\n",
    "export GEMINI_API_KEY=\"your-api-key-here\"\n",
    "```\n",
    "\n",
    "**Option 2 - Set directly in notebook (if export doesn't work):**\n",
    "```python\n",
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"your-api-key-here\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from unifiedllm import LLM\n",
    "from unifiedllm.errors import MissingAPIKeyError, ProviderAPIError\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Set API key directly in notebook (if not already set via terminal)\n",
    "# Uncomment and add your key if the export command didn't work:\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Check if API key is set\n",
    "if not os.getenv(\"GEMINI_API_KEY\"):\n",
    "    print(\"⚠️  GEMINI_API_KEY environment variable is not set!\")\n",
    "    print(\"\")\n",
    "    print(\"Option 1 - Set via terminal:\")\n",
    "    print(\"  export GEMINI_API_KEY='your-api-key'\")\n",
    "    print(\"\")\n",
    "    print(\"Option 2 - Set in this notebook (uncomment line above):\")\n",
    "    print(\"  os.environ['GEMINI_API_KEY'] = 'your-api-key'\")\n",
    "else:\n",
    "    print(\"✅ GEMINI_API_KEY is set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Chat with Prompts\n",
    "\n",
    "The simplest way to use UnifiedLLM is with the `chat()` method and a text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM client\n",
    "llm = LLM(\n",
    "    provider=\"gemini\",\n",
    "    model=\"gemini-2.5-flash\",\n",
    ")\n",
    "\n",
    "# Send a simple chat prompt\n",
    "response = llm.chat(prompt=\"What is the capital of France?\")\n",
    "\n",
    "# Print the response text\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding ChatResponse Fields\n",
    "\n",
    "The `chat()` method returns a `ChatResponse` object with several useful fields:\n",
    "- `text`: The model's response text\n",
    "- `request_id`: Unique identifier for the request (if available)\n",
    "- `usage`: Token usage information (if available)\n",
    "- `raw`: The raw response from the provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.chat(prompt=\"Explain photosynthesis in one sentence.\")\n",
    "\n",
    "print(\"Response Text:\")\n",
    "print(response.text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(f\"Request ID: {response.request_id}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Usage might be None for some providers/configurations\n",
    "if response.usage:\n",
    "    print(f\"Token Usage: {response.usage}\")\n",
    "else:\n",
    "    print(\"Usage information not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message-Based Conversations\n",
    "\n",
    "For more control, you can use the message format. This is especially useful for multi-turn conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a user message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"I'm learning Python. What's a list?\"}\n",
    "]\n",
    "\n",
    "response = llm.chat(messages=messages)\n",
    "print(\"Assistant:\", response.text)\n",
    "\n",
    "# Continue the conversation by adding the model's response\n",
    "messages.append({\"role\": \"model\", \"content\": response.text})\n",
    "messages.append({\"role\": \"user\", \"content\": \"Can you give me a simple example?\"})\n",
    "\n",
    "response = llm.chat(messages=messages)\n",
    "print(\"\\nAssistant:\", response.text)\n",
    "\n",
    "# Add another turn\n",
    "messages.append({\"role\": \"model\", \"content\": response.text})\n",
    "messages.append({\"role\": \"user\", \"content\": \"How do I add items to it?\"})\n",
    "\n",
    "response = llm.chat(messages=messages)\n",
    "print(\"\\nAssistant:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts\n",
    "\n",
    "System prompts help set the behavior and personality of the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a system prompt\n",
    "llm.system_prompt(\"You are a helpful teaching assistant who explains concepts simply and concisely. Always use analogies.\")\n",
    "\n",
    "response = llm.chat(prompt=\"What is recursion?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Options\n",
    "\n",
    "You can control model behavior with configuration parameters like `temperature` and `max_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model\n",
    "# temperature: controls randomness (0.0 = deterministic, 1.0 = creative)\n",
    "# max_tokens: limits response length\n",
    "llm.config(temperature=0.7, max_tokens=100)\n",
    "\n",
    "response = llm.chat(prompt=\"Write a short poem about coding.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with lower temperature for more focused responses\n",
    "llm.config(temperature=0.2, max_tokens=150)\n",
    "\n",
    "response = llm.chat(prompt=\"List 3 benefits of using Python.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling\n",
    "\n",
    "It's important to handle potential errors when working with LLM APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Handling missing API key error\n",
    "try:\n",
    "    # This would fail if api_key is explicitly set to an empty string\n",
    "    # and environment variable is not set\n",
    "    test_llm = LLM(provider=\"gemini\", model=\"gemini-1.5-flash\", api_key=\"\")\n",
    "    response = test_llm.chat(prompt=\"Hello\")\n",
    "except MissingAPIKeyError as e:\n",
    "    print(f\"❌ API Key Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Handling provider API errors\n",
    "# This demonstrates the pattern - actual errors depend on API responses\n",
    "try:\n",
    "    response = llm.chat(prompt=\"What is machine learning?\")\n",
    "    print(\"✅ Request successful!\")\n",
    "    print(response.text[:100] + \"...\")  # Print first 100 chars\n",
    "except ProviderAPIError as e:\n",
    "    print(f\"❌ Provider API Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Unexpected Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned the basics of using UnifiedLLM with Gemini:\n",
    "- ✅ Simple prompt-based chat\n",
    "- ✅ Message-based conversations\n",
    "- ✅ System prompts for behavior control\n",
    "- ✅ Configuration options (temperature, max_tokens)\n",
    "- ✅ Error handling patterns\n",
    "\n",
    "**Next steps:**\n",
    "- Check out `multi_turn_conversation.ipynb` to learn about building conversational apps\n",
    "- See `provider_comparison.ipynb` to learn how to switch between different LLM providers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".unifiedllm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
