{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Provider Comparison with UnifiedLLM\n",
    "\n",
    "One of the key benefits of UnifiedLLM is the ability to switch between different LLM providers using the same code interface.\n",
    "\n",
    "**Topics covered:**\n",
    "- Using the same code with different providers\n",
    "- Comparing responses from Gemini, Anthropic, and OpenAI\n",
    "- Handling missing API keys gracefully\n",
    "- Understanding provider differences\n",
    "\n",
    "**Supported providers:**\n",
    "- üî∑ **Gemini** (Google) - Free tier available\n",
    "- üü£ **Claude** (Anthropic) - Pay-as-you-go\n",
    "- üü¢ **GPT** (OpenAI) - Pay-as-you-go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### API Keys\n",
    "You'll need API keys for the providers you want to test:\n",
    "\n",
    "- **Gemini**: `GEMINI_API_KEY` - Get at [Google AI Studio](https://aistudio.google.com/app/apikey)\n",
    "- **Anthropic**: `ANTHROPIC_API_KEY` - Get at [Anthropic Console](https://console.anthropic.com/)\n",
    "- **OpenAI**: `OPENAI_API_KEY` - Get at [OpenAI Platform](https://platform.openai.com/api-keys)\n",
    "\n",
    "**Option 1 - Set via terminal (recommended):**\n",
    "```bash\n",
    "export GEMINI_API_KEY=\"your-google-key\"\n",
    "export ANTHROPIC_API_KEY=\"your-anthropic-key\"\n",
    "export OPENAI_API_KEY=\"your-openai-key\"\n",
    "```\n",
    "\n",
    "**Option 2 - Set directly in notebook (if export doesn't work):**\n",
    "```python\n",
    "import os\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"your-google-key\"\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "```\n",
    "\n",
    "**Note:** This notebook will skip providers whose API keys are not set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unifiedllm import LLM\n",
    "from unifiedllm.errors import MissingAPIKeyError, ProviderAPIError\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Set API keys directly in notebook (if not already set via terminal)\n",
    "# Uncomment and add your keys if the export command didn't work:\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"your-google-key\"\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\n",
    "\n",
    "# Check which API keys are available\n",
    "print(\"API Key Status:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "keys_status = {\n",
    "    \"GEMINI_API_KEY\": os.getenv(\"GEMINI_API_KEY\"),\n",
    "    \"ANTHROPIC_API_KEY\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")\n",
    "}\n",
    "\n",
    "for key_name, key_value in keys_status.items():\n",
    "    status = \"‚úÖ Set\" if key_value else \"‚ùå Not set\"\n",
    "    print(f\"{key_name}: {status}\")\n",
    "\n",
    "print(\"\\nProviders with missing keys will be skipped in comparisons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Shared Prompt\n",
    "\n",
    "Let's use the same prompt across all providers to compare their responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared prompt for comparison\n",
    "test_prompt = \"Explain the concept of machine learning in exactly 2 sentences.\"\n",
    "\n",
    "# Alternative: use messages format\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function for Provider Testing\n",
    "\n",
    "This function will handle API calls and gracefully skip providers with missing keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_provider(provider_name, model, env_var_name, prompt=None, messages=None):\n",
    "    \"\"\"\n",
    "    Test a specific provider and return results.\n",
    "    \n",
    "    Args:\n",
    "        provider_name: Name of the provider (e.g., \"gemini\", \"anthropic\", \"openai\")\n",
    "        model: Model name for the provider\n",
    "        env_var_name: Environment variable name for the API key\n",
    "        prompt: Optional prompt string\n",
    "        messages: Optional messages list\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with results or error information\n",
    "    \"\"\"\n",
    "    # Check if API key is set\n",
    "    api_key = os.getenv(env_var_name)\n",
    "    \n",
    "    if not api_key:\n",
    "        return {\n",
    "            \"provider\": provider_name,\n",
    "            \"model\": model,\n",
    "            \"status\": \"skipped\",\n",
    "            \"reason\": f\"{env_var_name} not set\",\n",
    "            \"text\": None,\n",
    "            \"request_id\": None,\n",
    "            \"usage\": None\n",
    "        }\n",
    "    \n",
    "    # Try to call the provider\n",
    "    try:\n",
    "        llm = LLM(provider=provider_name, model=model, api_key=None)\n",
    "        \n",
    "        if prompt:\n",
    "            response = llm.chat(prompt=prompt)\n",
    "        elif messages:\n",
    "            response = llm.chat(messages=messages)\n",
    "        else:\n",
    "            raise ValueError(\"Must provide either prompt or messages\")\n",
    "        \n",
    "        return {\n",
    "            \"provider\": provider_name,\n",
    "            \"model\": model,\n",
    "            \"status\": \"success\",\n",
    "            \"text\": response.text,\n",
    "            \"request_id\": response.request_id,\n",
    "            \"usage\": response.usage\n",
    "        }\n",
    "    \n",
    "    except MissingAPIKeyError as e:\n",
    "        return {\n",
    "            \"provider\": provider_name,\n",
    "            \"model\": model,\n",
    "            \"status\": \"error\",\n",
    "            \"reason\": f\"Missing API key: {e}\",\n",
    "            \"text\": None,\n",
    "            \"request_id\": None,\n",
    "            \"usage\": None\n",
    "        }\n",
    "    \n",
    "    except ProviderAPIError as e:\n",
    "        return {\n",
    "            \"provider\": provider_name,\n",
    "            \"model\": model,\n",
    "            \"status\": \"error\",\n",
    "            \"reason\": f\"API error: {e}\",\n",
    "            \"text\": None,\n",
    "            \"request_id\": None,\n",
    "            \"usage\": None\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"provider\": provider_name,\n",
    "            \"model\": model,\n",
    "            \"status\": \"error\",\n",
    "            \"reason\": f\"Unexpected error: {e}\",\n",
    "            \"text\": None,\n",
    "            \"request_id\": None,\n",
    "            \"usage\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Comparison Across Providers\n",
    "\n",
    "Let's test the same prompt with all three providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define provider configurations\n",
    "providers_config = [\n",
    "    {\n",
    "        \"provider\": \"gemini\",\n",
    "        \"model\": \"gemini-1.5-flash\",\n",
    "        \"env_var\": \"GOOGLE_API_KEY\",\n",
    "        \"display_name\": \"üî∑ Gemini (Google)\"\n",
    "    },\n",
    "    {\n",
    "        \"provider\": \"anthropic\",\n",
    "        \"model\": \"claude-3-5-sonnet-20241022\",\n",
    "        \"env_var\": \"ANTHROPIC_API_KEY\",\n",
    "        \"display_name\": \"üü£ Claude (Anthropic)\"\n",
    "    },\n",
    "    {\n",
    "        \"provider\": \"openai\",\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"env_var\": \"OPENAI_API_KEY\",\n",
    "        \"display_name\": \"üü¢ GPT (OpenAI)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Run comparison\n",
    "results = []\n",
    "\n",
    "print(\"Running provider comparison...\\n\")\n",
    "\n",
    "for config in providers_config:\n",
    "    print(f\"Testing {config['display_name']}...\")\n",
    "    result = run_provider(\n",
    "        provider_name=config[\"provider\"],\n",
    "        model=config[\"model\"],\n",
    "        env_var_name=config[\"env_var\"],\n",
    "        prompt=test_prompt\n",
    "    )\n",
    "    result[\"display_name\"] = config[\"display_name\"]\n",
    "    results.append(result)\n",
    "\n",
    "print(\"\\n‚úÖ Comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_comparison(results):\n",
    "    \"\"\"\n",
    "    Display comparison results in a readable format.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PROVIDER COMPARISON RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"\\n{result['display_name']}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Model: {result['model']}\")\n",
    "        print(f\"Status: {result['status']}\")\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            print(f\"\\nResponse:\")\n",
    "            print(result['text'])\n",
    "            print(f\"\\nRequest ID: {result['request_id'] if result['request_id'] else 'N/A'}\")\n",
    "            \n",
    "            if result['usage']:\n",
    "                print(f\"Token Usage: {result['usage']}\")\n",
    "            else:\n",
    "                print(\"Token Usage: Not available\")\n",
    "        \n",
    "        elif result['status'] == 'skipped':\n",
    "            print(f\"‚ö†Ô∏è  Skipped: {result['reason']}\")\n",
    "        \n",
    "        elif result['status'] == 'error':\n",
    "            print(f\"‚ùå Error: {result['reason']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Display the results\n",
    "display_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_table(results):\n",
    "    \"\"\"\n",
    "    Create a simple text-based summary table.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"QUICK SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Provider':<25} {'Status':<15} {'Response Length':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for result in results:\n",
    "        status_icon = {\n",
    "            \"success\": \"‚úÖ\",\n",
    "            \"skipped\": \"‚ö†Ô∏è\",\n",
    "            \"error\": \"‚ùå\"\n",
    "        }.get(result['status'], \"‚ùì\")\n",
    "        \n",
    "        response_length = len(result['text']) if result['text'] else 0\n",
    "        \n",
    "        print(f\"{result['display_name']:<25} {status_icon} {result['status']:<12} {response_length} chars\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "\n",
    "summary_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with Message Format\n",
    "\n",
    "Let's verify that the message-based approach works the same way across providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multi-turn conversation\n",
    "conversation_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 2 + 2?\"}\n",
    "]\n",
    "\n",
    "print(\"Testing message-based chat across providers...\\n\")\n",
    "\n",
    "for config in providers_config:\n",
    "    result = run_provider(\n",
    "        provider_name=config[\"provider\"],\n",
    "        model=config[\"model\"],\n",
    "        env_var_name=config[\"env_var\"],\n",
    "        messages=conversation_messages\n",
    "    )\n",
    "    \n",
    "    print(f\"{config['display_name']}:\")\n",
    "    if result['status'] == 'success':\n",
    "        print(f\"  Response: {result['text'][:100]}...\")\n",
    "    else:\n",
    "        print(f\"  {result['status'].upper()}: {result.get('reason', 'Unknown')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching Providers in Your Code\n",
    "\n",
    "Here's a practical example of how you might switch providers based on availability or preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_llm():\n",
    "    \"\"\"\n",
    "    Get the first available LLM provider.\n",
    "    Priority: Gemini -> Anthropic -> OpenAI\n",
    "    \"\"\"\n",
    "    providers = [\n",
    "        (\"gemini\", \"gemini-1.5-flash\", \"GEMINI_API_KEY\"),\n",
    "        (\"anthropic\", \"claude-3-5-sonnet-20241022\", \"ANTHROPIC_API_KEY\"),\n",
    "        (\"openai\", \"gpt-4o-mini\", \"OPENAI_API_KEY\")\n",
    "    ]\n",
    "    \n",
    "    for provider, model, env_var in providers:\n",
    "        if os.getenv(env_var):\n",
    "            print(f\"‚úÖ Using {provider} ({model})\")\n",
    "            return LLM(provider=provider, model=model, api_key=None)\n",
    "    \n",
    "    raise RuntimeError(\"No API keys found! Please set at least one: GEMINI_API_KEY, ANTHROPIC_API_KEY, or OPENAI_API_KEY\")\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    my_llm = get_available_llm()\n",
    "    response = my_llm.chat(prompt=\"Say hello!\")\n",
    "    print(f\"\\nResponse: {response.text}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"‚ùå {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Provider Differences\n",
    "\n",
    "While UnifiedLLM provides a consistent interface, it's important to understand that different providers have unique characteristics:\n",
    "\n",
    "### üî∑ Gemini (Google)\n",
    "- **Pros**: Free tier available, fast responses, good for prototyping\n",
    "- **Cons**: Smaller context window than some competitors\n",
    "- **Best for**: Learning, prototyping, cost-conscious projects\n",
    "\n",
    "### üü£ Claude (Anthropic)\n",
    "- **Pros**: Excellent reasoning, large context window, strong safety features\n",
    "- **Cons**: Pay-as-you-go only, can be verbose\n",
    "- **Best for**: Complex reasoning tasks, document analysis, safety-critical applications\n",
    "\n",
    "### üü¢ GPT (OpenAI)\n",
    "- **Pros**: Widely adopted, strong ecosystem, consistent performance\n",
    "- **Cons**: Pay-as-you-go only, rate limits on free tier\n",
    "- **Best for**: Production applications, well-established use cases\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "1. **Output Differences**: Different models will produce different responses to the same prompt, even if the quality is similar.\n",
    "\n",
    "2. **Pricing**: Each provider has different pricing models. Gemini offers a free tier, while Anthropic and OpenAI are pay-as-you-go.\n",
    "\n",
    "3. **Rate Limits**: Free tiers and paid tiers have different rate limits and quotas.\n",
    "\n",
    "4. **Response Format**: While UnifiedLLM normalizes the response format, the underlying raw responses differ.\n",
    "\n",
    "5. **Features**: Some providers support features others don't (e.g., function calling, vision capabilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Use Case: Fallback Pattern\n",
    "\n",
    "A common pattern is to use a fallback provider if your primary one fails or is rate-limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_fallback(prompt, providers_list):\n",
    "    \"\"\"\n",
    "    Try providers in order until one succeeds.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send\n",
    "        providers_list: List of (provider, model, env_var) tuples\n",
    "    \n",
    "    Returns:\n",
    "        ChatResponse or None\n",
    "    \"\"\"\n",
    "    for provider, model, env_var in providers_list:\n",
    "        if not os.getenv(env_var):\n",
    "            print(f\"‚ö†Ô∏è  Skipping {provider}: {env_var} not set\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            print(f\"Trying {provider}...\")\n",
    "            llm = LLM(provider=provider, model=model, api_key=None)\n",
    "            response = llm.chat(prompt=prompt)\n",
    "            print(f\"‚úÖ Success with {provider}!\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {provider} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"‚ùå All providers failed!\")\n",
    "    return None\n",
    "\n",
    "# Example usage\n",
    "fallback_providers = [\n",
    "    (\"gemini\", \"gemini-1.5-flash\", \"GEMINI_API_KEY\"),\n",
    "    (\"anthropic\", \"claude-3-5-sonnet-20241022\", \"ANTHROPIC_API_KEY\"),\n",
    "    (\"openai\", \"gpt-4o-mini\", \"OPENAI_API_KEY\")\n",
    "]\n",
    "\n",
    "response = chat_with_fallback(\"What is 5 + 7?\", fallback_providers)\n",
    "if response:\n",
    "    print(f\"\\nFinal response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've learned how to:\n",
    "- ‚úÖ Use the same code interface across multiple LLM providers\n",
    "- ‚úÖ Compare responses from different providers\n",
    "- ‚úÖ Handle missing API keys gracefully\n",
    "- ‚úÖ Implement fallback patterns for resilience\n",
    "- ‚úÖ Understand key differences between providers\n",
    "\n",
    "**Key takeaway:** UnifiedLLM makes it easy to switch providers without changing your code structure, giving you flexibility in choosing the best provider for your needs, budget, and use case.\n",
    "\n",
    "**Next steps:**\n",
    "- Experiment with your own prompts across providers\n",
    "- Build a simple app that lets users choose their preferred provider\n",
    "- Implement cost optimization by routing simple queries to cheaper models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
